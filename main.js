// main.js
console.log('âœ… main.js loaded');

import * as THREE     from 'three';
import { GLTFLoader } from 'three/examples/jsm/loaders/GLTFLoader.js';

const canvas = document.getElementById('avatar');
const scene  = new THREE.Scene();
let avatarRoot, mixer;

// â€”â€”â€” LIGHTING â€”â€”â€”
scene.add(new THREE.HemisphereLight(0xffffff, 0x666666, 1.0));
const keyLight = new THREE.DirectionalLight(0xffffff, 1.5);
keyLight.position.set(3, 10, 5); scene.add(keyLight);
const fillLight = new THREE.DirectionalLight(0xffffff, 1.0);
fillLight.position.set(-3, 5, 5); scene.add(fillLight);
const rimLight = new THREE.DirectionalLight(0xffffff, 0.7);
rimLight.position.set(0, 5, -5); scene.add(rimLight);
const groundLight = new THREE.PointLight(0xffffff, 0.5, 10);
groundLight.position.set(0, 0.3, 0); scene.add(groundLight);

// â€”â€”â€” CAMERA & RENDERER â€”â€”â€”
// Zoom in on the head: tighter FOV + higher Y focus
const camera = new THREE.PerspectiveCamera(
  35,
  window.innerWidth / window.innerHeight,
  0.1,
  100
);
camera.position.set(0, 1.8, 1.4);  // raise camera, pull in
camera.lookAt(0, 1.5, 0);          // look slightly above origin

const renderer = new THREE.WebGLRenderer({ canvas, alpha: true, antialias: true });
renderer.setSize(window.innerWidth, window.innerHeight);
window.addEventListener('resize', () => {
  camera.aspect = window.innerWidth/window.innerHeight;
  camera.updateProjectionMatrix();
  renderer.setSize(window.innerWidth, window.innerHeight);
});

// â€”â€”â€” LOAD & PREPARE AVATAR â€”â€”â€”
const loader = new GLTFLoader();
const morphMeshes = [];

loader.load(
  `./Animated_IDLE_coach.glb?cb=${Date.now()}`,
  gltf => {
    console.log('âœ… Animated_IDLE_coach.glb loaded');
    avatarRoot = gltf.scene;

    // Frame & center whole avatar
    const box    = new THREE.Box3().setFromObject(avatarRoot);
    const size   = box.getSize(new THREE.Vector3());
    const minY   = box.min.y;
    const center = box.getCenter(new THREE.Vector3());
    avatarRoot.position.sub(center);
    avatarRoot.position.y -= minY;
    avatarRoot.position.y += size.y * 0.1;  // lift so head is centered

    // Traverse to collect all morphâ€target meshes & drop arms
    avatarRoot.traverse(obj => {
      if (obj.isMesh && obj.morphTargetDictionary) {
        // Force-enable morphs on the material if missing
        if (!obj.material.morphTargets) {
          console.warn(`ðŸ”§ Enabling morphTargets on material ${obj.name}`);
          obj.material.morphTargets = true;
          obj.material.needsUpdate  = true;
        }
        // Log geometry data
        console.log(
          `ðŸ” [${obj.name}] morphAttrib count:`,
          obj.geometry.morphAttributes.position?.length,
          'relative?', obj.geometry.morphTargetsRelative
        );
        morphMeshes.push({
          mesh: obj,
          dict: obj.morphTargetDictionary
        });
      }
      if (obj.isBone && obj.name.toLowerCase().includes('upperarm')) {
        obj.rotation.z = obj.name.toLowerCase().includes('right')
          ? -Math.PI/2
          :  Math.PI/2;
      }
    });

    console.log(`âœ… Found ${morphMeshes.length} meshes with morph targets`);
    if (morphMeshes.length === 0) {
      console.error('âŒ No morph-target meshes detected! Check your export.');
    } else {
      console.log('ðŸ” Example morph keys:', Object.keys(morphMeshes[0].dict).slice(0,8));
    }

    scene.add(avatarRoot);
    mixer = new THREE.AnimationMixer(avatarRoot);
    startBlinking();

    if (gltf.animations && gltf.animations.length > 0) {
      const clip = gltf.animations[0];
      const action = mixer.clipAction(clip);
      action.setLoop(THREE.LoopRepeat);
      action.play();
      console.log(`ðŸŽ¬ Playing animation: ${clip.name}`);
      console.log(`â± Animation duration: ${clip.duration.toFixed(2)}s`);
    }

    // Delayed sanityâ€check blink + mouthOpen
    /*setTimeout(() => {
      console.log('ðŸ”§ Sanity test: blinking + mouthOpen');
      setExpression('eyeBlinkLeft',  1.0, 1500);
      setExpression('eyeBlinkRight', 1.0, 1500);
      setTimeout(() => setExpression('mouthOpen', 1.0, 1500), 1800);
    }, 800);*/
  },
  undefined,
  err => console.error('âŒ GLB load error:', err)
);

// â€”â€”â€” RENDER LOOP â€”â€”â€”
const clock = new THREE.Clock();
(function animate() {
  requestAnimationFrame(animate);

  const delta = clock.getDelta();
  mixer?.update(delta);

  renderer.render(scene, camera);
})();

// â€”â€”â€” HELPERS â€”â€”â€”
function setExpression(name, weight=1, duration=300) {
  morphMeshes.forEach(({ mesh, dict }) => {
    const idx = dict[name];
    if (idx != null) mesh.morphTargetInfluences[idx] = weight;
  });
  if (duration > 0) setTimeout(resetAll, duration);
}

function startBlinking() {
  (function blink(){
    setExpression('eyeBlinkLeft',  1.0, 150);
    setExpression('eyeBlinkRight', 1.0, 150);
    setTimeout(blink, 3000 + Math.random()*1500);
  })();
}

function startTalkingLoop() {
  let open = true;
  const talkInterval = setInterval(() => {
    setExpression('mouthOpen', open ? 1 : 0, 200);
    open = !open;
  }, 200);

  // Store it in window so we can stop it later
  window._talkingInterval = talkInterval;
}
function stopTalkingLoop() {
  clearInterval(window._talkingInterval);
  resetAll();
}

function resetAll() {
  morphMeshes.forEach(({ mesh }) => {
    mesh.morphTargetInfluences.fill(0);
  });
}

// expose for manual console testing
window.setExpression = setExpression;
window.resetAll      = resetAll;
window.startBlinking = startBlinking;

// â€”â€”â€” FLUTTER BRIDGE â€”â€”â€”
window.receiveFromFlutter = async ({ text }) => {
  console.log('â–¶ï¸ receiveFromFlutter:', text);

  // Facial emotion cues
  if (/[!?]$/.test(text.trim())) {
    setExpression('browOuterUpLeft',  1, 800);
    setExpression('browOuterUpRight', 1, 800);
  } else if (text.toLowerCase().includes('sorry')) {
    setExpression('mouthFrownLeft',  0.8, 800);
    setExpression('mouthFrownRight', 0.8, 800);
  } else {
    setExpression('mouthSmile', 0.6, 800);
  }

  // Lip-sync via Web Speech API
  return new Promise(resolve => {
    startTalkingLoop();
    const utter = new SpeechSynthesisUtterance(text);
    utter.lang = 'en-US';

    utter.onboundary = () => {
      const visNames = ['viseme_O','viseme_aa','mouthOpen'];
      morphMeshes.forEach(({ mesh, dict }) => {
        const key = visNames.find(n => dict[n] != null);
        const idx = dict[key] ?? 0;
        mesh.morphTargetInfluences[idx] = 1;
      });
      setTimeout(resetAll, 100);
    };

    utter.onend = () => {
      stopTalkingLoop();
      resetAll();
      resolve();
    };

    speechSynthesis.speak(utter);
  });
};
